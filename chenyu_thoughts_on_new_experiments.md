# idea 1

NOTE TO SELF: if this works, we can build a annotator pipeline to detect inconsistent data using the forward and backward path.
the goal is the detect the ambiguity / conflict / error.
the goal is to beat the state of the art.
IDEA: maybe we can try to do semantic alignment.
IDEA: maybe we can try to do cross-modal alignment.

# idea 2

TODO: another experiment we can run is using log prob as the representation of uncertainty.
link: https://openai.com/index/teaching-models-to-express-their-uncertainty-in-words/
